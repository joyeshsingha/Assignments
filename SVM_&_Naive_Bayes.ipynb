{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. What is Information Gain, and how is it used in Decision Trees?**  \n",
        "\n",
        "Information Gain is a metric used to decide which feature to split on at each node in a decision tree.\n",
        "It measures the reduction in entropy (uncertainty) after a dataset is split on a feature.\n",
        "\n",
        "Information Gain=Entropy(parent)−∑Weighted Entropy(children)\n",
        "\n",
        "Use in Decision Trees:\n",
        "\n",
        "The feature with the highest Information Gain is chosen for splitting.\n",
        "\n",
        "Used mainly in ID3 and C4.5 algorithms.\n",
        "\n",
        "Helps create purer child nodes.\n",
        "\n",
        "---\n",
        "\n",
        "**2. Difference between Gini Impurity and Entropy**  \n",
        "\n",
        "1. Computation:\n",
        "Gini Impurity is faster to compute because it does not use logarithms.\n",
        "Entropy is slower since it involves log calculations.\n",
        "\n",
        "2. Range and Sensitivity:\n",
        "Gini Impurity is less sensitive to changes in class probabilities.\n",
        "Entropy is more sensitive and gives higher weight to rare classes.\n",
        "\n",
        "3. Usage:\n",
        "Gini Impurity is commonly used in the CART algorithm and in sklearn.\n",
        "Entropy is mainly used in ID3 and C4.5 decision tree algorithms.\n",
        "\n",
        "---\n",
        "\n",
        "**3. What is Pre-Pruning in Decision Trees?**\n",
        "\n",
        "Pre-pruning is a technique that stops tree growth early to prevent overfitting.\n",
        "\n",
        "\n",
        "Common pre-pruning conditions:\n",
        "\n",
        "1.Maximum tree depth\n",
        "\n",
        "2.Minimum samples per split\n",
        "\n",
        "3.Minimum information gain\n",
        "\n",
        "4.Maximum number of leaf nodes\n",
        "\n",
        "Advantages:\n",
        "\n",
        "1.Reduces overfitting\n",
        "\n",
        "2.Faster training\n",
        "\n",
        "3.Simpler trees\n",
        "\n",
        "---\n",
        "\n",
        "**5. What is a Support Vector Machine (SVM)?**  \n",
        "\n",
        "SVM is a supervised learning algorithm that finds the optimal hyperplane separating classes with the maximum margin.\n",
        "\n",
        "Key points:\n",
        "\n",
        "Works for classification and regression\n",
        "\n",
        "Effective in high-dimensional spaces\n",
        "\n",
        "Uses support vectors (critical data points)\n",
        "\n",
        "---\n",
        "\n",
        "**6. What is the Kernel Trick in SVM?**  \n",
        "\n",
        "The Kernel Trick allows SVMs to solve non-linear problems by mapping data into a higher-dimensional space without explicitly computing the transformation.\n",
        "\n",
        "Common kernels:\n",
        "\n",
        "Linear\n",
        "\n",
        "Polynomial\n",
        "\n",
        "RBF (Gaussian)\n",
        "\n",
        "Sigmoid\n",
        "\n",
        "---\n",
        "\n",
        "**8. What is the Naïve Bayes classifier and why is it called “Naïve”?**  \n",
        "\n",
        "Naïve Bayes is a probabilistic classifier based on Bayes’ Theorem.\n",
        "\n",
        "It is called “Naïve” because it assumes that all features are conditionally independent, which is rarely true in real data.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Fast\n",
        "\n",
        "Works well with small datasets\n",
        "\n",
        "Performs well in text classification\n",
        "\n",
        "---\n",
        "\n",
        "**9. Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes.**  \n",
        "\n",
        "Gaussian Naïve Bayes:\n",
        "Gaussian Naïve Bayes is used for continuous numerical data and assumes that features follow a normal (Gaussian) distribution. It is commonly applied in medical, sensor, and real-valued datasets.\n",
        "\n",
        "Multinomial Naïve Bayes:\n",
        "Multinomial Naïve Bayes is used for discrete count data, such as word frequencies in text documents. It is widely used in text classification and NLP tasks like spam detection.\n",
        "\n",
        "Bernoulli Naïve Bayes:\n",
        "Bernoulli Naïve Bayes is used for binary features (0 or 1) that represent the presence or absence of a feature. It works well when feature occurrence matters more than frequency, such as binary word vectors.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "jk_CI9QB7KxZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practial Question"
      ],
      "metadata": {
        "id": "FwPr4WNDcJb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.Write a Python program to train a Decision Tree Classifier\n",
        "# using Gini Impurity as the criterion and print the feature importances (practical).\n",
        "# Hint: Use criterion='gini' in DecisionTreeClassifier and access .feature_importances_.\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Train Decision Tree with Gini Impurity\n",
        "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\", model.feature_importances_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFEPApkDRQ6Q",
        "outputId": "3cc04672-f536-472a-8d6b-f27843af5a22"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances: [0.01333333 0.         0.56405596 0.42261071]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.\n",
        "# Hint:Use SVC(kernel='linear') and SVC(kernel='rbf'), then compare accuracy scores after fitting on the same dataset.\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Linear Kernel SVM\n",
        "linear_svm = SVC(kernel='linear')\n",
        "linear_svm.fit(X_train, y_train)\n",
        "linear_acc = accuracy_score(y_test, linear_svm.predict(X_test))\n",
        "\n",
        "# RBF Kernel SVM\n",
        "rbf_svm = SVC(kernel='rbf')\n",
        "rbf_svm.fit(X_train, y_train)\n",
        "rbf_acc = accuracy_score(y_test, rbf_svm.predict(X_test))\n",
        "\n",
        "print(\"Linear SVM Accuracy:\", linear_acc)\n",
        "print(\"RBF SVM Accuracy:\", rbf_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ARQcX4arXkk",
        "outputId": "bca44fdf-b205-495c-d90b-e525875db4af"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear SVM Accuracy: 0.9814814814814815\n",
            "RBF SVM Accuracy: 0.7592592592592593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy.\n",
        "# Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from sklearn.datasets.\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train Gaussian Naive Bayes\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, model.predict(X_test))\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8qvfVqurmeO",
        "outputId": "8247bbe8-e350-4be7-f3ec-630664dfc3ff"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9415204678362573\n"
          ]
        }
      ]
    }
  ]
}