{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?**\n",
        "\n",
        "K-Nearest Neighbors (KNN) is a non-parametric, instance-based learning algorithm\n",
        "that makes predictions based on the similarity between data points.\n",
        "\n",
        "It works by:\n",
        "\n",
        "Finding the K nearest data points using a distance metric such as Euclidean distance\n",
        "\n",
        "Assigning the most frequent class among neighbors for classification\n",
        "\n",
        "Predicting the average of neighbor values for regression\n",
        "\n",
        "---\n",
        "\n",
        "**2. What is the Curse of Dimensionality and how does it affect KNN performance?**\n",
        "\n",
        "The Curse of Dimensionality refers to the challenges that arise when the number of\n",
        "features in a dataset becomes very large.\n",
        "\n",
        "It affects KNN by:\n",
        "\n",
        "Making data points sparse in high-dimensional space\n",
        "\n",
        "Reducing the effectiveness of distance measures\n",
        "\n",
        "Increasing computational cost and reducing accuracy\n",
        "\n",
        "---\n",
        "\n",
        "**3. What is Principal Component Analysis (PCA)? How is it different from feature selection?**\n",
        "\n",
        "Principal Component Analysis (PCA) is a dimensionality reduction technique that\n",
        "transforms original features into a smaller set of new, uncorrelated variables\n",
        "called principal components.\n",
        "\n",
        "PCA is different from feature selection because:\n",
        "\n",
        "PCA creates new features by combining original ones\n",
        "\n",
        "Feature selection keeps a subset of the original features without transformation\n",
        "\n",
        "---\n",
        "\n",
        "**4. What are eigenvalues and eigenvectors in PCA, and why are they important?**\n",
        "\n",
        "Eigenvectors represent the directions of maximum variance in the data,\n",
        "while eigenvalues represent the amount of variance captured by those directions.\n",
        "\n",
        "They are important because:\n",
        "\n",
        "Eigenvectors define the principal components\n",
        "\n",
        "Eigenvalues help decide which components to keep\n",
        "\n",
        "---\n",
        "\n",
        "**5. How do KNN and PCA complement each other when applied in a single pipeline?**\n",
        "\n",
        "PCA reduces dimensionality and removes noise from the data,\n",
        "making distance calculations more meaningful.\n",
        "\n",
        "KNN benefits from this reduced feature space by achieving\n",
        "better accuracy and faster computation.\n",
        "\n",
        "---\n",
        "\n",
        "**10. You are working with a high-dimensional gene expression dataset to classify cancer types. Explain your approach.**\n",
        "\n",
        "PCA would be used to reduce thousands of gene features into a smaller number\n",
        "of principal components, removing noise and redundancy.\n",
        "\n",
        "The number of components would be chosen based on explained variance,\n",
        "typically retaining 90â€“95% of the total variance.\n",
        "\n",
        "KNN would then be applied on the reduced feature space for classification.\n",
        "\n",
        "Model performance would be evaluated using cross-validation and metrics such as\n",
        "accuracy, precision, recall, and ROC-AUC.\n",
        "\n",
        "This pipeline reduces overfitting, improves generalization, and provides a\n",
        "robust solution for real-world biomedical data with high dimensionality\n",
        "and limited samples.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "jk_CI9QB7KxZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practial Questions"
      ],
      "metadata": {
        "id": "FwPr4WNDcJb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.Train a KNN Classifier on the Wine dataset with and without feature scaling.\n",
        "# Compare model accuracy in both cases.\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Without scaling\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "acc_no_scale = accuracy_score(y_test, knn.predict(X_test))\n",
        "\n",
        "# With scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "acc_scaled = accuracy_score(y_test, knn.predict(X_test_scaled))\n",
        "\n",
        "print(\"Accuracy without scaling:\", acc_no_scale)\n",
        "print(\"Accuracy with scaling:\", acc_scaled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFEPApkDRQ6Q",
        "outputId": "d0c6644b-3560-41ae-8bd8-ff188bdbd615"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.7407407407407407\n",
            "Accuracy with scaling: 0.9629629629629629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7 .Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component.\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Scale data\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "# PCA\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "print(\"Explained Variance Ratio:\")\n",
        "print(pca.explained_variance_ratio_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkYkl2s_NXkP",
        "outputId": "15e2e515-e452-4b45-bb10-1cb7493cede9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance Ratio:\n",
            "[0.36198848 0.1920749  0.11123631 0.0706903  0.06563294 0.04935823\n",
            " 0.04238679 0.02680749 0.02222153 0.01930019 0.01736836 0.01298233\n",
            " 0.00795215]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare\n",
        "#  the accuracy with the original dataset.\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# PCA with 2 components\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "X_train_pca, X_test_pca, y_train, y_test = train_test_split(\n",
        "    X_pca, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_pca, y_train)\n",
        "acc_pca = accuracy_score(y_test, knn.predict(X_test_pca))\n",
        "\n",
        "print(\"Accuracy with PCA:\", acc_pca)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-F9Z2_hNcJh",
        "outputId": "f80d6c1f-4ec5-47b1-9531-acc90bcf73c4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with PCA: 0.9814814814814815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Train a KNN Classifier with different distance metrics (euclidean, manhattan) on the scaled Wine dataset and compare the results.\n",
        "\n",
        "knn_euclidean = KNeighborsClassifier(metric='euclidean')\n",
        "knn_manhattan = KNeighborsClassifier(metric='manhattan')\n",
        "\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "\n",
        "acc_eu = accuracy_score(y_test, knn_euclidean.predict(X_test_scaled))\n",
        "acc_man = accuracy_score(y_test, knn_manhattan.predict(X_test_scaled))\n",
        "\n",
        "print(\"Euclidean Accuracy:\", acc_eu)\n",
        "print(\"Manhattan Accuracy:\", acc_man)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4CfeL9MNjBH",
        "outputId": "c58af2ea-7f5c-4d1c-baf8-fb4b1ead0e0c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Euclidean Accuracy: 0.9629629629629629\n",
            "Manhattan Accuracy: 0.9629629629629629\n"
          ]
        }
      ]
    }
  ]
}