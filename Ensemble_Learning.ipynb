{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. What is Ensemble Learning in machine learning? Explain the key idea behind it.**  \n",
        "\n",
        "Ensemble Learning combines multiple models to improve accuracy and robustness. Key idea: multiple weak learners together form a stronger model, reducing overfitting and improving generalization. Common methods: Bagging, Boosting, Stacking.\n",
        "\n",
        "---\n",
        "\n",
        "**2. What is the difference between Bagging and Boosting?**  \n",
        "\n",
        "Bagging trains multiple models in parallel using random bootstrap samples, while Boosting trains models sequentially by giving more weight to misclassified samples.\n",
        "\n",
        "\n",
        "Bagging mainly reduces variance and overfitting, whereas Boosting mainly reduces bias and improves accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "**3.What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?**\n",
        "\n",
        "Bootstrap sampling randomly selects data with replacement to create subsets. Each tree in Bagging trains on a different subset, introducing diversity and reducing variance.\n",
        "\n",
        "---\n",
        "\n",
        "**4. What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?**  \n",
        "\n",
        "OOB samples are the data not included in a bootstrap sample. The OOB score uses these samples to estimate model accuracy without a separate test set.\n",
        "\n",
        "---\n",
        "\n",
        "**5. Compare feature importance analysis in a single Decision Tree vs. a Random Forest.**  \n",
        "\n",
        "Decision Tree: Feature importance based on split reduction; unstable.\n",
        "\n",
        "Random Forest: Average importance across trees; more robust and reliable.\n",
        "\n",
        "---\n",
        "\n",
        "**10. You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data. You decide to use ensemble techniques to increase model performance. Explain your step-by-step approach.**  \n",
        "\n",
        "Step-by-step approach:\n",
        "\n",
        "\n",
        "Choose Bagging or Boosting:\n",
        "\n",
        "If data is noisy → Bagging (reduces variance).\n",
        "If data is clean with complex patterns → Boosting (reduces bias, improves accuracy).\n",
        "\n",
        "\n",
        "Handle Overfitting:\n",
        "\n",
        "Use regularization (e.g., max_depth, min_samples_leaf).\n",
        "Limit number of estimators in Boosting.\n",
        "Use cross-validation to monitor performance.\n",
        "\n",
        "Select Base Models:\n",
        "Decision Trees are common as base learners.\n",
        "Can also use Logistic Regression or SVM for Bagging.\n",
        "\n",
        "\n",
        "Evaluate Performance:\n",
        "Use k-fold cross-validation.\n",
        "Metrics: Accuracy, Precision, Recall, F1-score, ROC-AUC.\n",
        "\n",
        "\n",
        "Justification:\n",
        "Ensemble learning combines multiple models → reduces errors and variance.\n",
        "Improves robustness and reliability in predicting loan defaults → better decision-making\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "jk_CI9QB7KxZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practial Questions"
      ],
      "metadata": {
        "id": "FwPr4WNDcJb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6: Write a Python program to:\n",
        "# ● Load the Breast Cancer dataset using sklearn.datasets.load_breast_cancer()\n",
        "# ● Train a Random Forest Classifier\n",
        "# ● Print the top 5 most important features based on feature importance scores\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load data\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Feature importances\n",
        "importances = pd.Series(rf.feature_importances_, index=feature_names)\n",
        "top5 = importances.sort_values(ascending=False).head(5)\n",
        "print(\"Top 5 important features:\\n\", top5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFEPApkDRQ6Q",
        "outputId": "aaed962f-6501-4c6c-a3d5-f9c4e719076d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 important features:\n",
            " worst area              0.139357\n",
            "worst concave points    0.132225\n",
            "mean concave points     0.107046\n",
            "worst radius            0.082848\n",
            "worst perimeter         0.080850\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7: Write a Python program to:\n",
        "# ● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "# ● Evaluate its accuracy and compare with a single Decision Tree\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "acc_dt = accuracy_score(y_test, y_pred_dt)\n",
        "\n",
        "# Bagging Classifier (FIXED)\n",
        "bag = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bag.fit(X_train, y_train)\n",
        "y_pred_bag = bag.predict(X_test)\n",
        "acc_bag = accuracy_score(y_test, y_pred_bag)\n",
        "\n",
        "print(f\"Decision Tree Accuracy: {acc_dt:.3f}\")\n",
        "print(f\"Bagging Accuracy: {acc_bag:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ARQcX4arXkk",
        "outputId": "7922585e-5a79-466d-89ef-ad5a6ce65cf4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.000\n",
            "Bagging Accuracy: 1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Write a Python program to:\n",
        "# ● Train a Random Forest Classifier\n",
        "# ● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "# ● Print the best parameters and final accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Hyperparameter tuning\n",
        "param_grid = {'n_estimators': [50,100,150], 'max_depth': [2,3,4,None]}\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "grid = GridSearchCV(rf, param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "best_rf = grid.best_estimator_\n",
        "y_pred = best_rf.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(f\"Accuracy: {acc:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8qvfVqurmeO",
        "outputId": "13f0726f-bc32-4e2d-e2bf-43927bbf0c92"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 2, 'n_estimators': 150}\n",
            "Accuracy: 1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9: Write a Python program to:\n",
        "# ● Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset\n",
        "# ● Compare their Mean Squared Errors (MSE)\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load data\n",
        "data = fetch_california_housing()\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data.data, data.target, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Bagging Regressor (FIXED)\n",
        "bag_reg = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bag_reg.fit(X_train, y_train)\n",
        "y_pred_bag = bag_reg.predict(X_test)\n",
        "mse_bag = mean_squared_error(y_test, y_pred_bag)\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "y_pred_rf = rf_reg.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "print(f\"Bagging Regressor MSE: {mse_bag:.3f}\")\n",
        "print(f\"Random Forest MSE: {mse_rf:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_IRC9a3Em34",
        "outputId": "a4cba273-d806-4537-9f1b-efd03865fa69"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE: 0.258\n",
            "Random Forest MSE: 0.257\n"
          ]
        }
      ]
    }
  ]
}